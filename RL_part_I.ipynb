{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYs6LMEbNqoQ"
      },
      "source": [
        "# RL coursework, part I (20 pts total)\n",
        "---\n",
        "\n",
        "**Name:** Harry Yao\n",
        "\n",
        "**SN:** 20012409\n",
        "\n",
        "---\n",
        "\n",
        "**Due date:** *April 10th, 2025*\n",
        "\n",
        "---\n",
        "Standard UCL policy (including grade deductions) automatically applies for any late submissions.\n",
        "\n",
        "\n",
        "## How to submit\n",
        "\n",
        "When you have completed the exercises and everything has finished running, click on 'File' in the menu-bar and then 'Download .ipynb'. This file must be submitted to Moodle named as **`<studentnumber>_RL_part1.ipynb`** before the deadline above, where `<studentnumber>` is your student number."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tJro6oEQOAb"
      },
      "source": [
        "----\n",
        "**Reminder of copyrights**\n",
        "\n",
        "Copyrights protect this code/content and distribution or usages of it (or parts of it) without permission is prohibited. This includes uploading it and usage of it in training in any LLMs systems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9v_SYckYfv5G"
      },
      "source": [
        "**Context**\n",
        "\n",
        "In this assignment, we will take a first look at learning decisions from data.  For this, we will use the multi-armed bandit framework.\n",
        "\n",
        "**Background reading**\n",
        "\n",
        "* Sutton and Barto (2018), Chapters 1 to 6\n",
        "* Lecture slides"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNuohp44N00i"
      },
      "source": [
        "**Overview of this assignment**\n",
        "\n",
        "A) You will use Python to implement several bandit algorithms.\n",
        "\n",
        "B) You will then run these algorithms on a multi-armed Bernoulli bandit problem, and answer question about their empirical performance.\n",
        "\n",
        "C) You will then be asked to reason about the behaviour of different algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztQEQvnKh2t6"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Run each of the cells below, until you reach the next section **Basic Agents**. You do not have to read or understand the code in the **Setup** section.  After running the cells, feel free to fold away the **Setup** section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzYtxi8Wh5SJ"
      },
      "outputs": [],
      "source": [
        "# Import Useful Libraries\n",
        "\n",
        "import collections\n",
        "from functools import partial\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns  # Import Seaborn\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=1)\n",
        "sns.set_theme(style=\"white\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YP97bVN3NuG8"
      },
      "outputs": [],
      "source": [
        "class BernoulliBandit(object):\n",
        "  \"\"\"A stationary multi-armed Bernoulli bandit.\"\"\"\n",
        "\n",
        "  def __init__(self, success_probabilities, success_reward=1., fail_reward=0.):\n",
        "    \"\"\"Constructor of a stationary Bernoulli bandit.\n",
        "\n",
        "    Args:\n",
        "      success_probabilities: A list or numpy array containing the probabilities,\n",
        "          for each of the arms, of providing a success reward.\n",
        "      success_reward: The reward on success (default: 1.)\n",
        "      fail_reward: The reward on failure (default: 0.)\n",
        "    \"\"\"\n",
        "    self._probs = success_probabilities\n",
        "    self._number_of_arms = len(self._probs)\n",
        "    self._s = success_reward\n",
        "    self._f = fail_reward\n",
        "\n",
        "    ps = np.array(success_probabilities)\n",
        "    self._values = ps * success_reward + (1 - ps) * fail_reward\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"The step function.\n",
        "\n",
        "    Args:\n",
        "      action: An integer or np.int32 that specifies which arm to pull.\n",
        "\n",
        "    Returns:\n",
        "      A reward sampled according to the success probability of the selected arm.\n",
        "\n",
        "    Raises:\n",
        "      ValueError: when the provided action is out of bounds.\n",
        "    \"\"\"\n",
        "    if action < 0 or action >= self._number_of_arms:\n",
        "      raise ValueError('Action {} is out of bounds for a '\n",
        "                       '{}-armed bandit'.format(action, self._number_of_arms))\n",
        "\n",
        "    success = bool(np.random.random() < self._probs[action])\n",
        "    reward = success * self._s + (not success) * self._f\n",
        "    return reward\n",
        "\n",
        "  def regret(self, action):\n",
        "    \"\"\"Computes the regret for the given action.\"\"\"\n",
        "    return self._values.max() - self._values[action]\n",
        "\n",
        "  def optimal_value(self):\n",
        "    \"\"\"Computes the regret for the given action.\"\"\"\n",
        "    return self._values.max()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYxNiGcRxbd0"
      },
      "outputs": [],
      "source": [
        "class NonStationaryBandit(object):\n",
        "  \"\"\"A non-stationary multi-armed Bernoulli bandit.\"\"\"\n",
        "\n",
        "  def __init__(self, success_probabilities,\n",
        "               success_reward=1., fail_reward=0., change_point=800,\n",
        "               change_is_good=True):\n",
        "    \"\"\"Constructor of a non-stationary Bernoulli bandit.\n",
        "\n",
        "    Args:\n",
        "      success_probabilities: A list or numpy array containing the probabilities,\n",
        "          for each of the arms, of providing a success reward.\n",
        "      success_reward: The reward on success (default: 1.)\n",
        "      fail_reward: The reward on failure (default: 0.)\n",
        "      change_point: The number of steps before the rewards change.\n",
        "      change_is_good: Whether the rewards go up (if True), or flip (if False).\n",
        "    \"\"\"\n",
        "    self._probs = success_probabilities\n",
        "    self._number_of_arms = len(self._probs)\n",
        "    self._s = success_reward\n",
        "    self._f = fail_reward\n",
        "    self._change_point = change_point\n",
        "    self._change_is_good = change_is_good\n",
        "    self._number_of_steps_so_far = 0\n",
        "\n",
        "    ps = np.array(success_probabilities)\n",
        "    self._values = ps * success_reward + (1 - ps) * fail_reward\n",
        "\n",
        "  def step(self, action):\n",
        "    \"\"\"The step function.\n",
        "\n",
        "    Args:\n",
        "      action: An integer or np.int32 that specifies which arm to pull.\n",
        "\n",
        "    Returns:\n",
        "      A reward sampled according to the success probability of the selected arm.\n",
        "\n",
        "    Raises:\n",
        "      ValueError: when the provided action is out of bounds.\n",
        "    \"\"\"\n",
        "    if action < 0 or action >= self._number_of_arms:\n",
        "      raise ValueError('Action {} is out of bounds for a '\n",
        "                       '{}-armed bandit'.format(action, self._number_of_arms))\n",
        "\n",
        "    self._number_of_steps_so_far += 1\n",
        "    success = bool(np.random.random() < self._probs[action])\n",
        "    reward = success * self._s + (not success) * self._f\n",
        "\n",
        "    if self._number_of_steps_so_far == self._change_point:\n",
        "      # After some number of steps, the rewards are inverted\n",
        "      #\n",
        "      #  ``The past was alterable. The past never had been altered. Oceania was\n",
        "      #    at war with Eastasia. Oceania had always been at war with Eastasia.``\n",
        "      #            - 1984, Orwell (1949).\n",
        "      reward_dif = (self._s - self._f)\n",
        "      if self._change_is_good:\n",
        "        self._f = self._s + reward_dif\n",
        "      else:\n",
        "        self._s -= reward_dif\n",
        "        self._f += reward_dif\n",
        "\n",
        "      # Recompute expected values when the rewards change\n",
        "      ps = np.array(self._probs)\n",
        "      self._values = ps * self._s + (1 - ps) * self._f\n",
        "\n",
        "    return reward\n",
        "\n",
        "  def regret(self, action):\n",
        "    \"\"\"Computes the regret for the given action.\"\"\"\n",
        "    return self._values.max() - self._values[action]\n",
        "\n",
        "  def optimal_value(self):\n",
        "    \"\"\"Computes the regret for the given action.\"\"\"\n",
        "    return self._values.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DU7KGFJ0DN-H"
      },
      "outputs": [],
      "source": [
        "# Helper functions\n",
        "\n",
        "def smooth(array, smoothing_horizon=100., initial_value=0.):\n",
        "  \"\"\"Smoothing function for plotting.\"\"\"\n",
        "  smoothed_array = []\n",
        "  value = initial_value\n",
        "  b = 1./smoothing_horizon\n",
        "  m = 1.\n",
        "  for x in array:\n",
        "    m *= 1. - b\n",
        "    lr = b/(1 - m)\n",
        "    value += lr*(x - value)\n",
        "    smoothed_array.append(value)\n",
        "  return np.array(smoothed_array)\n",
        "\n",
        "def plot(algs, plot_data, repetitions=30):\n",
        "  \"\"\"Plot results of a bandit experiment.\"\"\"\n",
        "  algs_per_row = 4\n",
        "  n_algs = len(algs)\n",
        "  n_rows = (n_algs - 2)//algs_per_row + 1\n",
        "  fig = plt.figure(figsize=(10, 4*n_rows))\n",
        "  fig.subplots_adjust(wspace=0.3, hspace=0.35)\n",
        "  clrs = ['#000000', '#00bb88', '#0033ff', '#aa3399', '#ff6600']\n",
        "  lss = ['--', '-', '-', '-', '-']\n",
        "  for i, p in enumerate(plot_data):\n",
        "    for c in range(n_rows):\n",
        "      ax = fig.add_subplot(n_rows, len(plot_data), i + 1 + c*len(plot_data))\n",
        "      ax.grid(0)\n",
        "\n",
        "      current_algs = [algs[0]] + algs[c*algs_per_row + 1:(c + 1)*algs_per_row + 1]\n",
        "      for alg, clr, ls in zip(current_algs, clrs, lss):\n",
        "        data = p.data[alg.name]\n",
        "        m = smooth(np.mean(data, axis=0))\n",
        "        s = np.std(smooth(data.T).T, axis=0)/np.sqrt(repetitions)\n",
        "        if p.log_plot:\n",
        "          line = plt.semilogy(m, alpha=0.7, label=alg.name,\n",
        "                              color=clr, ls=ls, lw=3)[0]\n",
        "        else:\n",
        "          line = plt.plot(m, alpha=0.7, label=alg.name,\n",
        "                          color=clr, ls=ls, lw=3)[0]\n",
        "          plt.fill_between(range(len(m)), m + s, m - s,\n",
        "                           color=line.get_color(), alpha=0.2)\n",
        "      if p.opt_values is not None:\n",
        "        plt.plot(p.opt_values[current_algs[0].name][0], ':', alpha=0.5,\n",
        "                 label='optimal')\n",
        "\n",
        "      ax.set_facecolor('white')\n",
        "      ax.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\",\n",
        "                     labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\")\n",
        "      ax.spines[\"top\"].set_visible(False)\n",
        "      ax.spines[\"bottom\"].set(visible=True, color='black', lw=1)\n",
        "      ax.spines[\"right\"].set_visible(False)\n",
        "      ax.spines[\"left\"].set(visible=True, color='black', lw=1)\n",
        "      ax.get_xaxis().tick_bottom()\n",
        "      ax.get_yaxis().tick_left()\n",
        "\n",
        "      data = np.array([smooth(np.mean(d, axis=0)) for d in p.data.values()])\n",
        "\n",
        "      if p.log_plot:\n",
        "        start, end = calculate_lims(data, p.log_plot)\n",
        "        start = np.floor(np.log10(start))\n",
        "        end = np.ceil(np.log10(end))\n",
        "        ticks = [_*10**__\n",
        "                 for _ in [1., 2., 3., 5.]\n",
        "                 for __ in [-2., -1., 0.]]\n",
        "        labels = [r'${:1.2f}$'.format(_*10** __)\n",
        "                  for _ in [1, 2, 3, 5]\n",
        "                  for __ in [-2, -1, 0]]\n",
        "        plt.yticks(ticks, labels)\n",
        "      plt.ylim(calculate_lims(data, p.log_plot))\n",
        "      plt.locator_params(axis='x', nbins=4)\n",
        "\n",
        "      plt.title(p.title)\n",
        "      if i == len(plot_data) - 1:\n",
        "        plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
        "\n",
        "def run_experiment(bandit_constructor, algs, repetitions, number_of_steps):\n",
        "  \"\"\"Run multiple repetitions of a bandit experiment.\"\"\"\n",
        "  reward_dict = {}\n",
        "  regret_dict = {}\n",
        "  optimal_value_dict = {}\n",
        "\n",
        "  for alg in algs:\n",
        "    reward_dict[alg.name] = np.zeros((repetitions, number_of_steps))\n",
        "    regret_dict[alg.name] = np.zeros((repetitions, number_of_steps))\n",
        "    optimal_value_dict[alg.name] = np.zeros((repetitions, number_of_steps))\n",
        "\n",
        "    for _rep in range(repetitions):\n",
        "      bandit = bandit_constructor()\n",
        "      alg.reset()\n",
        "\n",
        "      action = None\n",
        "      reward = None\n",
        "      for _step in range(number_of_steps):\n",
        "        action = alg.step(action, reward)\n",
        "        reward = bandit.step(action)\n",
        "        regret = bandit.regret(action)\n",
        "        optimal_value = bandit.optimal_value()\n",
        "\n",
        "        reward_dict[alg.name][_rep, _step] = reward\n",
        "        regret_dict[alg.name][_rep, _step] = regret\n",
        "        optimal_value_dict[alg.name][_rep, _step] = optimal_value\n",
        "\n",
        "  return reward_dict, regret_dict, optimal_value_dict\n",
        "\n",
        "\n",
        "def train_agents(agents, number_of_arms, number_of_steps, repetitions=100,\n",
        "                 success_reward=1., fail_reward=0.,\n",
        "                 bandit_class=BernoulliBandit):\n",
        "\n",
        "  success_probabilities = np.arange(0.3, 0.7 + 1e-6, 0.4/(number_of_arms - 1))\n",
        "\n",
        "  bandit_constructor = partial(bandit_class,\n",
        "                               success_probabilities=success_probabilities,\n",
        "                               success_reward=success_reward,\n",
        "                               fail_reward=fail_reward)\n",
        "  rewards, regrets, opt_values = run_experiment(\n",
        "      bandit_constructor, agents, repetitions, number_of_steps)\n",
        "\n",
        "  smoothed_rewards = {}\n",
        "  for agent, rs in rewards.items():\n",
        "    smoothed_rewards[agent] = np.array(rs)\n",
        "\n",
        "  PlotData = collections.namedtuple('PlotData',\n",
        "                                    ['title', 'data', 'opt_values', 'log_plot'])\n",
        "  total_regrets = dict([(k, np.cumsum(v, axis=1)) for k, v in regrets.items()])\n",
        "  plot_data = [\n",
        "      PlotData(title='Smoothed rewards', data=smoothed_rewards,\n",
        "               opt_values=opt_values, log_plot=False),\n",
        "      PlotData(title='Current Regret', data=regrets, opt_values=None,\n",
        "               log_plot=True),\n",
        "      PlotData(title='Total Regret', data=total_regrets, opt_values=None,\n",
        "               log_plot=False),\n",
        "  ]\n",
        "\n",
        "  plot(agents, plot_data, repetitions)\n",
        "\n",
        "def calculate_lims(data, log_plot=False):\n",
        "  y_min = np.min(data)\n",
        "  y_max = np.max(data)\n",
        "  diff = y_max - y_min\n",
        "  if log_plot:\n",
        "    y_min = 0.9*y_min\n",
        "    y_max = 1.1*y_max\n",
        "  else:\n",
        "    y_min = y_min - 0.05*diff\n",
        "    y_max = y_max + 0.05*diff\n",
        "  return y_min, y_max\n",
        "\n",
        "def argmax(array):\n",
        "  \"\"\"Returns the maximal element, breaking ties randomly.\"\"\"\n",
        "  return np.random.choice(np.flatnonzero(array == array.max()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzpb_dGVjT0O"
      },
      "source": [
        "# A) Agent implementations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBHsuFyapu5r"
      },
      "source": [
        "\n",
        "All agents should be in pure Python/NumPy.\n",
        "\n",
        "You cannot use any AutoDiff packages (Jax, TF, PyTorch, etc.)\n",
        "\n",
        "Each agent, should implement the following methods:\n",
        "\n",
        "**`step(self, previous_action, reward)`:**\n",
        "\n",
        "Should update the statistics by updating the value for the previous_action towards the observed reward.\n",
        "\n",
        "(Note: make sure this can handle the case that previous_action=None, in which case no statistics should be updated.)\n",
        "\n",
        "(Hint: you can split this into two steps: 1. update values, 2. get new action.  Make sure you update the values before selecting a new action.)\n",
        "\n",
        "**`reset(self)`:**\n",
        "\n",
        "Resets statistics (should be equivalent to constructing a new agent from scratch).\n",
        "\n",
        "Make sure that the initial values (after a reset) are all zero.\n",
        "\n",
        "**`__init__(self, name, number_of_arms, *args)`:**\n",
        "\n",
        "The `__init__` should take at least an argument `number_of_arms`, and (potentially) agent specific args."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwUUBXgQ2MCk"
      },
      "source": [
        "## Example agent\n",
        "\n",
        "The following code block contains an example random agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPYlY9M22JOI"
      },
      "outputs": [],
      "source": [
        "class Random(object):\n",
        "  \"\"\"A random agent.\n",
        "\n",
        "  This agent returns an action between 0 and 'number_of_arms', uniformly at\n",
        "  random. The 'previous_action' argument of 'step' is ignored.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, name, number_of_arms):\n",
        "    \"\"\"Initialise the agent.\n",
        "\n",
        "    Sets the name to `random`, and stores the number of arms. (In multi-armed\n",
        "    bandits `arm` is just another word for `action`.)\n",
        "    \"\"\"\n",
        "    self._number_of_arms = number_of_arms\n",
        "    self.name = name\n",
        "\n",
        "  def step(self, unused_previous_action, unused_reward):\n",
        "    \"\"\"Returns a random action.\n",
        "\n",
        "    The inputs are ignored, but this function still requires an action and a\n",
        "    reward, to have the same interface as other agents who may use these inputs\n",
        "    to learn.\n",
        "    \"\"\"\n",
        "    return np.random.randint(self._number_of_arms)\n",
        "\n",
        "  def reset(self):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDTyvlZsvSQq"
      },
      "source": [
        "\n",
        "## Q1 [2 pts]\n",
        "Implement a UCB agent.\n",
        "\n",
        "The `bonus_multiplier` is the parameter $c$ from the slides."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZM5NtZ3Q2X0F"
      },
      "outputs": [],
      "source": [
        "class UCB(object):\n",
        "  def __init__(self, name, number_of_arms, bonus_multiplier):\n",
        "    self._number_of_arms = number_of_arms\n",
        "    self._bonus_multiplier = bonus_multiplier\n",
        "    self.name = name\n",
        "    self.reset()\n",
        "\n",
        "  def step(self, previous_action, reward):\n",
        "    self.t += 1\n",
        "\n",
        "    if previous_action is not None:\n",
        "      self.counts[previous_action] += 1\n",
        "      self.values[previous_action] += (reward - self.values[previous_action]) / self.counts[previous_action]\n",
        "\n",
        "    for i in range(self._number_of_arms):\n",
        "      if self.counts[i] == 0:\n",
        "          return i\n",
        "\n",
        "    action = argmax(self.values + self._bonus_multiplier * np.sqrt(np.log(self.t) / self.counts))\n",
        "\n",
        "    return action\n",
        "\n",
        "  def reset(self):\n",
        "    self.counts = np.zeros(self._number_of_arms)\n",
        "    self.values = np.zeros(self._number_of_arms)\n",
        "    self.t = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqJxDegZtqXN"
      },
      "source": [
        "## Q2 [1 pt]\n",
        "Implement an $\\epsilon$-greedy agent.\n",
        "\n",
        "This agent should be able to support time-changing $\\epsilon$ schedules.\n",
        "\n",
        "Thus, your agent should accept both constants and callables as constructor argument `epsilon`; callables are used to decay the $\\epsilon$ parameter over time, for instance according to a polynomial schedule: $\\epsilon_t = t^{-\\eta}$ with $\\eta \\in [0, 1]$).\n",
        "\n",
        "\n",
        "If multiple actions have the same value, ties should be broken randomly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_1pB2p7146i"
      },
      "outputs": [],
      "source": [
        "class EpsilonGreedy(object):\n",
        "  \"\"\"An epsilon-greedy agent.\n",
        "\n",
        "  This agent returns an action between 0 and 'number_of_arms'; with probability\n",
        "  `(1-epsilon)` it chooses the action with the highest estimated value, while\n",
        "  with probability `epsilon` it samples an action uniformly at random.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, name, number_of_arms, epsilon=0.1):\n",
        "    self.name = name\n",
        "    self._number_of_arms = number_of_arms\n",
        "    self._epsilon = epsilon\n",
        "    self.reset()\n",
        "\n",
        "  def step(self, previous_action, reward):\n",
        "    \"\"\"Update the learnt statistics and return an action.\n",
        "\n",
        "    A single call to step uses the provided reward to update the value of the\n",
        "    taken action (which is also provided as an input), and returns an action.\n",
        "    The action is either uniformly random (with probability epsilon), or greedy\n",
        "    (with probability 1 - epsilon).\n",
        "\n",
        "    If the input action is None (typically on the first call to step), then no\n",
        "    statistics are updated, but an action is still returned.\n",
        "    \"\"\"\n",
        "    self.t += 1\n",
        "    if previous_action is not None:\n",
        "      self.counts[previous_action] += 1\n",
        "      self.values[previous_action] += (reward - self.values[previous_action]) / self.counts[previous_action]\n",
        "\n",
        "    epsilon = self._epsilon(self.t) if callable(self._epsilon) else self._epsilon\n",
        "\n",
        "    if np.random.random() < epsilon:\n",
        "      action = np.random.randint(self._number_of_arms)\n",
        "    else:\n",
        "      action = argmax(self.values)\n",
        "\n",
        "    return action\n",
        "\n",
        "  def reset(self):\n",
        "    self.counts = np.zeros(self._number_of_arms)\n",
        "    self.values = np.zeros(self._number_of_arms)\n",
        "    self.t = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enKI7uNjI1Ym"
      },
      "source": [
        "## Q3 [2 pts]\n",
        "Implement a REINFORCE agent.\n",
        "\n",
        "While `softmax` distributions are a common parametrization for policies over discrete action-spaces, they are not the only choice. In this exercise we ask you to implement REINFORCE with the `square-max` policy parameterization. With this parametrisation the probabilities depend on the action preferences $p(\\cdot)$ according to the expression:\n",
        "\n",
        "$$\\pi(a) = \\frac{p(a)^2}{\\sum_b p(b)^2}\\,.$$\n",
        "\n",
        "Implement a REINFORCE policy-gradient method for updating the preferences under this policy distribution. The action preferences are stored separately, so that for each action $a$ the preference $p(a)$ is a single value that you directly update.\n",
        "\n",
        "The agent should be able to use a baseline or not (as defined in the constructor). The `step_size` parameter $\\alpha$ used to update the policy must also be configurable in the constructor.\n",
        "\n",
        "The baseline should track the average reward so far, using the same `step_size` used to update the policy.\n",
        "\n",
        "The `step_size` and whether or not a baseline is used are defined in the constructor by feeding additional arguments in place of `...` below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqcC-OZq9bP7"
      },
      "outputs": [],
      "source": [
        "class REINFORCE(object):\n",
        "  def __init__(self, name, number_of_arms, step_size=0.1, baseline=False):\n",
        "    self.name = name\n",
        "    self._number_of_arms = number_of_arms\n",
        "    self._step_size = step_size\n",
        "    self._baseline = baseline\n",
        "    self.reset()\n",
        "\n",
        "  def step(self, previous_action, reward):\n",
        "    self.t += 1\n",
        "    if previous_action is not None:\n",
        "      grad = -2*self.performances/np.sum(self.performances**2)\n",
        "      grad[previous_action] += 2/self.performances[previous_action]\n",
        "      advantage = reward - self.average_reward\n",
        "      self.performances += self._step_size * advantage * grad\n",
        "      if self._baseline:\n",
        "        self.average_reward += self._step_size * (reward - self.average_reward)\n",
        "      squared = self.performances**2\n",
        "      squared = self.performances ** 2\n",
        "      total = np.sum(squared)\n",
        "      if total == 0 or np.isnan(total):\n",
        "          policy = np.ones(self._number_of_arms) / self._number_of_arms\n",
        "      else:\n",
        "          policy = squared / total\n",
        "      action = np.random.choice(self._number_of_arms, p=policy)\n",
        "    else:\n",
        "      action = np.random.randint(self._number_of_arms)\n",
        "    return action\n",
        "\n",
        "  def reset(self):\n",
        "    self.performances = np.ones(self._number_of_arms)  # 避免除 0\n",
        "\n",
        "    self.average_reward = 0\n",
        "    self.t = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jZsPzCmDxAh"
      },
      "source": [
        "# B) Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQkk8sMxE0N4"
      },
      "source": [
        "**Run the cell below to train the agents and generate the plots for the first experiment.**\n",
        "\n",
        "Trains the agents on a Bernoulli bandit problem with 5 arms,\n",
        "with a reward on success of 1, and a reward on failure of 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06P4AfZw1GSH"
      },
      "source": [
        "## Experiment 1: Bernoulli bandit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loe_YN7Tv8yY"
      },
      "outputs": [],
      "source": [
        "%%capture experiment1\n",
        "\n",
        "number_of_arms = 5\n",
        "number_of_steps = 1000\n",
        "\n",
        "agents = [\n",
        "    Random(\n",
        "        \"random\",\n",
        "        number_of_arms),\n",
        "    EpsilonGreedy(\n",
        "        r\"$\\epsilon$-greedy with $\\epsilon=0$\",\n",
        "        number_of_arms,\n",
        "        epsilon=0.),\n",
        "    EpsilonGreedy(\n",
        "        r\"$\\epsilon$-greedy with $\\epsilon=0.1$\",\n",
        "        number_of_arms,\n",
        "        epsilon=0.1),\n",
        "    EpsilonGreedy(\n",
        "        r\"$\\epsilon$-greedy with $\\epsilon_t=1/t$\",\n",
        "        number_of_arms,\n",
        "        epsilon=lambda t: 1./t),\n",
        "    EpsilonGreedy(\n",
        "        r\"$\\epsilon$-greedy with $\\epsilon_t=1/\\sqrt{t}$\",\n",
        "        number_of_arms,\n",
        "        epsilon=lambda t: 1./t**0.5),\n",
        "    UCB(\"UCB\",\n",
        "        number_of_arms,\n",
        "        bonus_multiplier=1/np.sqrt(2)),\n",
        "    REINFORCE(\n",
        "        r\"REINFORCE, $\\alpha=0.1$\",\n",
        "        number_of_arms,\n",
        "        step_size=0.1,\n",
        "        baseline=False),\n",
        "    REINFORCE(\n",
        "        r\"REINFORCE with baseline, $\\alpha=0.1$\",\n",
        "        number_of_arms,\n",
        "        step_size=0.1,\n",
        "        baseline=True),\n",
        "]\n",
        "\n",
        "train_agents(agents, number_of_arms, number_of_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnbpeszVp04-"
      },
      "outputs": [],
      "source": [
        "experiment1.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWyaMm-sjz9a"
      },
      "source": [
        "## Q4 [4 pts total]\n",
        "(Answer inline in the markdown below each question, **within this text cell**.)\n",
        "\n",
        "**[2 pts]**\n",
        "For each algorithm in the plots above, explain whether or not we should be expected it to be good in general, in terms of total regret.\n",
        "\n",
        "1. We should not expect random policy will perform well, as it didn't record and adapt for improvement overtime, therefore in experiment with highest regret.\n",
        "2. Fixed epsilon with 0.1 is expexted to performance as average, constant expolaring without deducing cause unnessasary exploration and regret.\n",
        "3. Decaying epsilon strategy should perform better, early expolration increase confidence for gradually exploit, which leads to lower regret in experiment.\n",
        "4. UCB is theoratically expected to have the best performance. Efficient balancing between exploration and exploitation leads to constant low experiment regret.\n",
        "5. REINFORCE also is expected to perform well. Baseline version achieve reduced variance and better regret than the version without baseline. Thier performance also varies by its different varioubles setup.\n",
        "\n",
        "**[2 pts]** Explain the relative ranking of the $\\epsilon$-greedy algorithms in this experiment.\n",
        "\n",
        "1. ε = 1/√t performance best. Slow exploration decay leads to enough early and mid-stage exploration with better identification of best arem and lowest regret.\n",
        "2. ε = 1/t, is the second best. Fast decaying causing exploration stop too early that the choice of the arm might not the best.\n",
        "3. ε = 0.1 performs worse, as constant exploration rate leads to unnessasary regret.\n",
        "4. ε = 0 is the worst. Lacking of exploration. If the start is bad, the whole experiment is bad."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YO5NDaPGDsp"
      },
      "source": [
        "## Experiment 2: reward = 0 on success, reward = -1 on failure.\n",
        "\n",
        "**Run the cell below to train the agents and generate the plots for the second experiment.**\n",
        "Reruns experiment 1 but on a different bernoulli bandit problem with 5 arms,\n",
        "with a reward on success of 0, and a reward on failure of -1.\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "7cvJf4WzmJXK"
      },
      "outputs": [],
      "source": [
        "%%capture experiment2\n",
        "number_of_arms = 5\n",
        "number_of_steps = 1000\n",
        "\n",
        "train_agents(agents, number_of_arms, number_of_steps,\n",
        "             success_reward=0., fail_reward=-1.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5RXnvnFLOGa"
      },
      "outputs": [],
      "source": [
        "experiment2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GOe5RDsnj4J"
      },
      "source": [
        "## Q5 [2 pts]\n",
        "For each algorithm, note whether the performance changed significantly compared to the **experiment 1**, and explain why it did or did not.\n",
        "\n",
        "(Use at most two sentences per algorithm).\n",
        "\n",
        "1. ε-greedy performances mainly the same as experiment1, as the reward shifted as a whole effecting all actions equally, ration of exploration and exploitation remains the same.\n",
        "2. UCB still perform strong and consistent, as it based on reward difference, shift won't change its decision making.\n",
        "3. REINFORCE shows similar trends but with overall downwards reward shifting, as the policy update depends on reward and the baseline tracks the new average reward.\n",
        "4. Random policy got no affection as it is not reward-based."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRDmw4nyFI73"
      },
      "source": [
        "## Run the following cells"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Drtsr8Cc1OWl"
      },
      "source": [
        "## Experiment 3: Non-stationary bandit\n",
        " * Reward on `failure` changes from 0 to +2.\n",
        " * Reward on `success` remains at +1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4RseDt-MCkq"
      },
      "outputs": [],
      "source": [
        "%%capture experiment3\n",
        "\n",
        "number_of_arms = 3\n",
        "number_of_steps = 1984\n",
        "agents = [\n",
        "    Random(\n",
        "        \"random\",\n",
        "        number_of_arms),\n",
        "    EpsilonGreedy(\n",
        "        r\"$\\epsilon$-greedy with $\\epsilon=0.1$\",\n",
        "        number_of_arms,\n",
        "        epsilon=0.1),\n",
        "    EpsilonGreedy(\n",
        "        r\"$\\epsilon$-greedy with $\\epsilon_t=1/\\sqrt{t}$\",\n",
        "        number_of_arms,\n",
        "        epsilon=lambda t: 1./t**0.5),\n",
        "    UCB(\"UCB\",\n",
        "        number_of_arms,\n",
        "        bonus_multiplier=1/np.sqrt(2)),\n",
        "    REINFORCE(\n",
        "        r\"REINFORCE with baseline, $\\alpha=0.1$\",\n",
        "        number_of_arms,\n",
        "        step_size=0.1,\n",
        "        baseline=True),\n",
        "\n",
        "]\n",
        "\n",
        "roving_bandit_class = partial(NonStationaryBandit, change_is_good=True)\n",
        "train_agents(agents, number_of_arms, number_of_steps,\n",
        "             bandit_class=roving_bandit_class)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sErI1V9h1ScE"
      },
      "source": [
        "## Experiment 4: Non-stationary bandit\n",
        " * Reward on `failure` changes from 0 to +1.\n",
        " * Reward on `success` changes from +1 to 0.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yT_mZxCIAfg9"
      },
      "outputs": [],
      "source": [
        "%%capture experiment4\n",
        "\n",
        "number_of_arms = 3\n",
        "number_of_steps = 1984\n",
        "\n",
        "\n",
        "roving_bandit_class = partial(NonStationaryBandit, change_is_good=False)\n",
        "train_agents(agents, number_of_arms, number_of_steps,\n",
        "             bandit_class=roving_bandit_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aWjHNwbEsDJ"
      },
      "outputs": [],
      "source": [
        "experiment3.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s703_VCICCTL"
      },
      "outputs": [],
      "source": [
        "experiment4.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x84zO7DK2_t"
      },
      "source": [
        "## Q6 [9 pts total]\n",
        "\n",
        "Observe the reward and regret curves above.  After 800 steps, the rewards change. In **experiment 3** `success` continues to yield a reward of +1, but `failure` changes from a reward of 0 to a reward of +2.  In **experiment 4**, `success` is now worth 0 and `failure` is worth +1.\n",
        "\n",
        "Below, we ask for explanations.  Answer each question briefly, using at most three sentences per question.\n",
        "\n",
        "**[2 pts]** In **experiment 3** explain the ranking in current regret after the change in rewards for all algorithms.\n",
        "\n",
        "> As ranked in experiment:\n",
        "1. REINFORCE with basedline performs the best. Policy gradient made quick adjustment on its policy, and the baseline reduces change, making learning stable and adapted to new faliure reward quick.\n",
        "2. UCB the second. Exploration bounus made it re-evaluate arms after reward change.\n",
        "3. ε-greedy with ε = 1/√t performs in mid as exploration after reward change made it adapt to new reward but slow.\n",
        "4. Constant ε-greedy performs bad as constant exploration won't notice the change of reward.\n",
        "5. Random the worst, as it's not reward-related.\n",
        "\n",
        "**[2 pts]** In **experiment 4** explain the ranking in current regret after the change in rewards for all algorithms.\n",
        "\n",
        "> As ranked in experiment:\n",
        "1. REINFORCE with basedline performs the best. Policy gradient made quick adjustment on its policy, and the baseline helps adjust smoothly, making learning stable and adapted to new reward quick.\n",
        "2. ε-greedy with ε = 1/√t performs in mid as exploration after reward change made it adapt to new reward but slow.\n",
        "3. Constant ε-greedy performs bad as constant exploration won't notice the change of reward.\n",
        "2. UCB became poor here. Reliance on historical data made it fails to adapt as good arms become bad.\n",
        "5. Random the worst, as it's not reward-related.\n",
        "\n",
        "**[2 pts]** Explain how and why the current-regret curve for UCB in **experiment 3** differs from the curve in **experiment 4**.\n",
        "\n",
        "> Cause UCB based on historical data. In experiment3, failuare rewards increasing and continious exploration made UCB finding new rewarding arms quick, keeping low regret. In experiment4, good arm became bad, UCB kept using outdated info, causing high regret.\n",
        "\n",
        "**[3 pts]** In general, if rewards can be non-stationary, and we don't know the exact nature of the non-stationarity, how could we modify UCB to perform better?   Be specific and concise.\n",
        "\n",
        "> 1. Reset after detecting change in reward, if detection is possible.\n",
        "2. Using sliding window on rewards, keep observation not outdated. This way UCB will adapt to new reward, keeping low regret.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGhirrkCChAZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}