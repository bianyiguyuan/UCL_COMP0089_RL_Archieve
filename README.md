# UCL_COMP0089_RL_Archieve

This repository contains the complete coursework for **COMP0089 Reinforcement Learning** at University College London (UCL), completed in 2025.

It is organized into four parts, each corresponding to a different reinforcement learning topic. Each notebook is fully annotated with both implementation and theoretical analysis.

---

## Contents

| File | Description |
|------|-------------|
| `RL_part_I.ipynb` | **Bandit Problems** — Implementation of ε-greedy and UCB algorithms, with regret analysis. |
| `RL_part_II.ipynb` | **Tabular RL (TD/Q-learning)** — Temporal-difference learning, off-policy evaluation, and variance analysis. |
| `RL_part_III.ipynb` | **Function Approximation** — Linear and non-linear value function approximation, divergence analysis. |
| `RL_part_IV.ipynb` | **Policy Gradient Methods** — REINFORCE and Actor-Critic implementations, baseline tuning and advantage estimation. |
| `requirements.txt` | Python dependencies required to run all notebooks. |

---

## Setup Instructions

To run this project locally:

1. Clone this repository:

```bash
git clone https://github.com/yourusername/UCL_COMP0089_RL_Archieve.git
cd UCL_COMP0089_RL_Archieve


pip install -r requirements.txt
