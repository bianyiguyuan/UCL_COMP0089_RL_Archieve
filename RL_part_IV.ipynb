{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYs6LMEbNqoQ"
      },
      "source": [
        "# RL coursework, part IV (30 pts in total)\n",
        "\n",
        "---\n",
        "\n",
        "**Name:** Harry Yao\n",
        "\n",
        "**SN:** 20012409\n",
        "\n",
        "---\n",
        "\n",
        "**Due date:** *April 10th, 2025*\n",
        "\n",
        "---\n",
        "\n",
        "Standard UCL policy (including grade deductions) automatically applies for any late submissions.\n",
        "\n",
        "## How to submit\n",
        "\n",
        "When you have completed the exercises and everything has finished running, click on 'File' in the menu-bar and then 'Download .ipynb'. This file must be submitted to Moodle named as **`<studentnumber>_RL_part4.ipynb`** before the deadline above, where `<studentnumber>` is your student number.\n",
        "\n",
        "----\n",
        "**Reminder of copyrights**\n",
        "\n",
        "Copyrights protect this code/content and distribution or usages of it (or parts of it) without permission is prohibited. This includes uploading it and usage of it in training in any LLMs systems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNuohp44N00i"
      },
      "source": [
        "### Objectives\n",
        "\n",
        "#### Q.1: You will implement a number of off-policy multi-step return estimates, and answer questions about their accuracy.\n",
        "\n",
        "#### Q.2: You will be looking at other, TD-like, updates to learn the value function. You will be asked to investigate different properties of these: e.g. convergence properties, variance of updates. This is akin to a typical analysis one would undertaken when proposing a new update rule to learn value functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1p0fpbxQLyn"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ps5OnkPmDbMX"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "np.set_printoptions(precision=3, suppress=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thswfgXU_p05"
      },
      "source": [
        "## Section A: Multi-step + offpolicy in practice [11 points total]\n",
        "For many concrete algorithms, we need to combine multi-step updates with off-policy corrections.  The multi-step updates are necessary for efficient learning, while the off-policy corrections are necessary to learn about multiple things at once, or to correct for a distribution mismatch (e.g., when trying to perform a policy-gradient update from logged data).\n",
        "\n",
        "In this section, you will implement various different returns with off-policy corrections.  The next cell has two examples *without* corrections.  These examples compute equivalent returns, but compute those returns in different ways.  These are provided as reference implementations to help you.\n",
        "\n",
        "Note that the implementations both allow for immediate bootstrapping on the current state value. This is unconventional (most literature only allows the first bootstrapping to happen after the first step), but we will use this convention in all implementations below for consistency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHNH35SZYHBu"
      },
      "outputs": [],
      "source": [
        "#@title On-policy return computations\n",
        "\n",
        "def on_policy(observations, actions, pi, mu, rewards, discounts,\n",
        "              trace_parameter, v_fn):\n",
        "  \"\"\"Compute on-policy return recursively.\"\"\"\n",
        "  del mu  # The policy probabilities are ignored by this function\n",
        "  del pi\n",
        "  T = len(rewards)  # number of transitions\n",
        "  r = rewards\n",
        "  d = discounts\n",
        "  l = trace_parameter\n",
        "  v = np.array([v_fn(o) for o in observations])\n",
        "  G = np.zeros((T,))\n",
        "  # recurse backwards to calculate returns\n",
        "  for t in reversed(range(T)):\n",
        "    # There are T+1 observations, but only T rewards, and the indexing here\n",
        "    # for the rewards is off by one compared to the indexing in the slides\n",
        "    # and in Sutton & Barto.  In other words, r[t] == R_{t+1}.\n",
        "    if t == T - 1:\n",
        "      G[t] = r[t] + d[t]*v[t + 1]\n",
        "    else:\n",
        "      G[t] = r[t] + d[t]*((1 - l)*v[t + 1] + l*G[t + 1])\n",
        "  v = v[:-1]  # Remove (T+1)th observation before calculating the returns\n",
        "  return (1 - l)*v + l*G\n",
        "\n",
        "def on_policy_error_recursion(observations, actions, pi, mu, rewards, discounts,\n",
        "                              trace_parameter, v_fn):\n",
        "  del pi  # The target policy probabilities are ignored by this function\n",
        "  del mu  # The behaviour policy probabilities are ignored by this function\n",
        "  T = len(rewards)  # number of transitions\n",
        "  r = rewards\n",
        "  d = discounts\n",
        "  l = trace_parameter\n",
        "  v = np.array([v_fn(o) for o in observations])\n",
        "  errors = np.zeros((T,))\n",
        "\n",
        "  error = 0.\n",
        "  # recurse backwards to calculate errors\n",
        "  for t in reversed(range(T)):\n",
        "    error = r[t] + d[t]*v[t + 1] - v[t] + d[t]*l*error\n",
        "    errors[t] = error\n",
        "  v = v[:-1]  # Remove (T+1)th observation before calculating the returns\n",
        "  return v + l*errors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNXhobrYHeiy"
      },
      "source": [
        "### Q 1.1 [5 points]\n",
        "Implement the return functions below and run the cells below that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPzHHrmn5Tm7"
      },
      "outputs": [],
      "source": [
        "def full_importance_sampling(observations, actions, pi, mu, rewards, discounts, trace_parameter, v_fn):\n",
        "  \"\"\"\n",
        "  Compute off-policy return with full importance-sampling corrections, so that\n",
        "  the return G_t is corrected with the full importance-sampling correction of\n",
        "  the rest of the trajectory.\n",
        "  \"\"\"\n",
        "  T = len(rewards)  # number of transitions\n",
        "  r = rewards\n",
        "  d = discounts\n",
        "  l = trace_parameter\n",
        "  v = np.array([v_fn(o) for o in observations])\n",
        "  G = np.zeros((T,))\n",
        "  rho = pi/mu\n",
        "  # recurse backwards to calculate returns\n",
        "  for t in reversed(range(T)):\n",
        "    cumulative_rho = np.prod(rho[t:])\n",
        "    if t == T - 1:\n",
        "      G[t] = (r[t] + d[t]*v[t + 1]) * cumulative_rho\n",
        "    else:\n",
        "      G[t] = (r[t] + d[t]*((1 - l)*v[t + 1] + l*G[t + 1])) * cumulative_rho\n",
        "  v = v[:-1]  # Remove (T+1)th observation before calculating the returns\n",
        "  return (1 - l)*v + l*G\n",
        "\n",
        "def per_decision(observations, actions, pi, mu, rewards, discounts, trace_parameter, v_fn):\n",
        "  \"\"\"\n",
        "  Compute off-policy return with per-decision importance-sampling corrections.\n",
        "  \"\"\"\n",
        "  T = len(rewards)  # number of transitions\n",
        "  r = rewards\n",
        "  d = discounts\n",
        "  l = trace_parameter\n",
        "  v = np.array([v_fn(o) for o in observations])\n",
        "  rho = pi/mu\n",
        "  G = np.zeros((T,))\n",
        "  for t in reversed(range(T)):\n",
        "    if t == T - 1:\n",
        "      G[t] = r[t] + d[t] * v[t + 1] * rho[t]\n",
        "    else:\n",
        "      G[t] = r[t] + d[t] * ((1 - l) * v[t + 1] + l * G[t + 1]) * rho[t]\n",
        "  v = v[:-1]\n",
        "  return (1 - l)*v + l*G\n",
        "\n",
        "def control_variates(observations, actions, pi, mu, rewards, discounts, trace_parameter, v_fn):\n",
        "  \"\"\"\n",
        "  Compute off-policy return with\n",
        "  1. per-decision importance-sampling corrections, and\n",
        "  2. control variates\n",
        "  \"\"\"\n",
        "  T = len(rewards)  # number of transitions\n",
        "  r = rewards\n",
        "  d = discounts\n",
        "  l = trace_parameter\n",
        "  v = np.array([v_fn(o) for o in observations])\n",
        "  rho = pi/mu\n",
        "  G = np.zeros((T,))\n",
        "  for t in reversed(range(T)):\n",
        "    if t == T - 1:\n",
        "        G[t] = r[t] + d[t] * rho[t] * v[t + 1] - d[t] * (rho[t] - 1) * v[t + 1]\n",
        "    else:\n",
        "        G[t] = (r[t] + d[t] * rho[t] * ((1 - l) * v[t + 1] + l * G[t + 1]) - d[t] * (rho[t] - 1) * v[t + 1])\n",
        "  v = v[:-1]\n",
        "  return (1 - l)*v + l*G\n",
        "\n",
        "def adaptive_bootstrapping(observations, actions, pi, mu, rewards, discounts, trace_parameter, v_fn):\n",
        "  \"\"\"\n",
        "  Compute off-policy return with\n",
        "  1. per-decision importance-sampling corrections, and\n",
        "  2. control variates, and\n",
        "  3. adaptive bootstrapping.\n",
        "\n",
        "  Implement the adaptive bootstrapping with an *additional* trace parameter\n",
        "  lambda, such that lambda_t = lambda * min(1, 1/rho_t).\n",
        "  \"\"\"\n",
        "  T = len(rewards)  # number of transitions\n",
        "  r = rewards\n",
        "  d = discounts\n",
        "  lam = trace_parameter\n",
        "  v = np.array([v_fn(o) for o in observations])\n",
        "  rho = pi/mu\n",
        "  G = np.zeros((T,))\n",
        "  for t in reversed(range(T)):\n",
        "    lambda_t = lam * min(1, 1/rho[t])\n",
        "    if t == T - 1:\n",
        "       G[t] = r[t] + d[t] * rho[t] * v[t + 1] - d[t] * (rho[t] - 1) * v[t + 1]\n",
        "    else:\n",
        "        G[t] = (r[t] + d[t] * rho[t] * ((1 - lambda_t) * v[t + 1] + lambda_t * G[t + 1]) - d[t] * (rho[t] - 1) * v[t + 1])\n",
        "  v = v[:-1]\n",
        "  return (1 - lam) * v + lam * G\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EEHYK89ANIA"
      },
      "outputs": [],
      "source": [
        "#@title (Run, don't modify) Functions to generate experience, compute values\n",
        "MU_RIGHT = 0.5\n",
        "PI_RIGHT = 0.9\n",
        "NUMBER_OF_STEPS = 5\n",
        "DISCOUNT = 0.99\n",
        "\n",
        "def generate_experience():\n",
        "  r\"\"\"Generate experience trajectories from a tabular tree MDP.\n",
        "\n",
        "  This function will start in state 0, and will then generate actions according\n",
        "  to a uniformly random behaviour policy.  When A_t == 0, the action will be to\n",
        "  the left, with A_t==1, it will be to the right.  The states are nunmbered as\n",
        "  depicted below:\n",
        "          0\n",
        "         / \\\n",
        "        1   2\n",
        "       / \\ / \\\n",
        "      3   4   5\n",
        "         ...\n",
        "\n",
        "  Args:\n",
        "      number_of_steps: the number of total steps.\n",
        "      p_right: probability of the behaviour to go right.\n",
        "\n",
        "  Returns:\n",
        "      A dictionary with elements:\n",
        "        * observations (number_of_steps + 1 integers): the\n",
        "          observations are just the actual (integer) states\n",
        "        * actions (number_of_steps integers): actions per step\n",
        "        * rewards (number_of_steps scalars): rewards per step\n",
        "        * discounts (number_of_steps scalars): currently always 0.9,\n",
        "          except the last one which is zero\n",
        "        * mu (number_of_steps scalars): probability of selecting each\n",
        "          action according to the behavious policy\n",
        "        * pi (number_of_steps scalars): probability of selecting each\n",
        "          action according to the target policy (here p(1) = 0.9 and\n",
        "          p(0) = 0.1, where a==1 implies we go 'right')\n",
        "  \"\"\"\n",
        "  # generate actions\n",
        "  actions = np.array(np.random.random(NUMBER_OF_STEPS,) < MU_RIGHT,\n",
        "                     dtype=int)\n",
        "  s = 0\n",
        "  # compute resulting states\n",
        "  states = np.cumsum(np.arange(1, NUMBER_OF_STEPS + 1) + actions)\n",
        "  states = np.array([0] + list(states))  # add start state\n",
        "\n",
        "  # in this case, observations are just the real states\n",
        "  observations = states\n",
        "\n",
        "  # generate rewards\n",
        "  rewards     = 2.*actions - 1. # -1 for left, +1 for right,\n",
        "  rewards[-1] = np.sum(actions)  # extra final reward for going right\n",
        "\n",
        "  # compute discounts\n",
        "  discounts     = DISCOUNT * np.ones_like(rewards)\n",
        "  discounts[-1] = 0.  # final transition is terminal, has discount=0\n",
        "\n",
        "  # determine target and behaviour probabilities for the selected actions\n",
        "  pi = np.array([1. - PI_RIGHT, PI_RIGHT])[actions] # Target probabilities\n",
        "  mu = np.array([1. - MU_RIGHT, MU_RIGHT])[actions] # Behaviour probabilities\n",
        "\n",
        "  return dict(observations=observations,\n",
        "              actions=actions,\n",
        "              pi=pi,\n",
        "              mu=mu,\n",
        "              rewards=rewards,\n",
        "              discounts=discounts)\n",
        "\n",
        "def true_v(s, pi, number_of_steps):\n",
        "  \"\"\"Compute true state value recursively.\"\"\"\n",
        "  depth = int(np.floor((np.sqrt(1 + 8*s) - 1)/2))\n",
        "  position = int(s - depth*(depth+1)/2)\n",
        "  remaining_steps = number_of_steps - depth\n",
        "  final_reward = DISCOUNT**(remaining_steps-1)*(position + pi*remaining_steps)\n",
        "  reward_per_step = pi*(+1) + (1 - pi)*(-1)\n",
        "  discounted_steps = (1 - DISCOUNT**(remaining_steps - 1))/(1 - DISCOUNT)\n",
        "  reward_along_the_way = reward_per_step * discounted_steps\n",
        "  return reward_along_the_way + final_reward\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCFMUmOfRTqZ"
      },
      "outputs": [],
      "source": [
        "#@title Run experiment (don't modify)\n",
        "algs = ['on_policy', 'full_importance_sampling', 'per_decision', 'control_variates', 'adaptive_bootstrapping']\n",
        "\n",
        "# Precompute state values (for efficiency)\n",
        "N = NUMBER_OF_STEPS\n",
        "true_vs = [true_v(s, PI_RIGHT, N) for s in range((N+1)*(N+2)//2)]\n",
        "\n",
        "def random_v(iteration, s):\n",
        "  rng = np.random.RandomState(seed=s + iteration*10000)\n",
        "  return true_vs[s] + rng.normal(loc=0, scale=1.)  # Add fixed random noise\n",
        "\n",
        "def plot_errors(ax, errors):\n",
        "  errors = np.array(errors)\n",
        "  ax.violinplot(np.log10(errors), showextrema=False)\n",
        "  ax.plot(range(1, len(algs)+1), np.log10(errors).T,\n",
        "          '.', color='#667799', ms=7, alpha=0.2)\n",
        "  ax.plot(range(1, len(algs)+1), np.log10(np.mean(errors, axis=0)),\n",
        "          '.', color='#000000', ms=20)\n",
        "  ax.set_yticks(np.arange(-2, 5))\n",
        "  ax.set_yticklabels(10.**np.arange(-2, 5), fontsize=13)\n",
        "  ax.set_ylabel(\"Value error $(v(s_0) - v_{\\\\pi}(s_0))^2$\", fontsize=15)\n",
        "  ax.set_xticks(range(1, len(algs)+1))\n",
        "  ax.set_xticklabels(algs, fontsize=15, rotation=70)\n",
        "  ax.set_ylim(-1, 4)\n",
        "\n",
        "fig = plt.figure(figsize=(12, 8))\n",
        "\n",
        "errors = []\n",
        "estimates = []\n",
        "v0 = true_vs[0]\n",
        "for iteration in range(1000):\n",
        "  errors.append([])\n",
        "  estimates.append([])\n",
        "  trajectory = generate_experience()\n",
        "  for alg in algs:\n",
        "    estimate = eval(alg)(**trajectory,\n",
        "                        v_fn=lambda s: random_v(iteration, s),\n",
        "                        trace_parameter=0.9)\n",
        "    errors[-1].append((estimate[0] - v0)**2)\n",
        "print(np.mean(errors, axis=0))\n",
        "plot_errors(plt.gca(), errors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hlc4jctHHqv"
      },
      "source": [
        "Above, the distributions of mean squared value errors are shown, with the mean as a big black dot and the (1,000) individual return samples as small black dots.\n",
        "\n",
        "### Q 1.2 [3 points]\n",
        "Explain the ranking in terms of value error of the different return estimates.\n",
        "\n",
        "Rank from lowest to highest value error:\n",
        "\n",
        "1. adaptive_bootstrapping (lowest error)\n",
        "\n",
        "2. control_variates\n",
        "\n",
        "3. on_policy\n",
        "\n",
        "4. per_decision\n",
        "\n",
        "5. full_importance_sampling (highest error)\n",
        "\n",
        "Explaination:\n",
        "\n",
        "- Adaptive bootstrapping combines per-decision corrections, control variates and adjusts the bootstrapping factor dynamically based on importance weights, achieving the best trade-off between bias and variance, result as the lowest error.\n",
        "\n",
        "- Control variates reduce variance by subtracting expected value under behavior policy, improving over both per-decision and on-policy estimators.\n",
        "\n",
        "- On-policy avoids importance sampling entirely. However, lacking correction of off-policy data leads to bias.\n",
        "\n",
        "- Per-decision importance sampling applies correction locally at each step for improvement, but still suffers by variance.\n",
        "\n",
        "- Full importance sampling has the highest variance due to the product of importance weights over time, which can explode easily and destabilize learning. This mechansim leads to the worst performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0Uk1d9b4CPc"
      },
      "source": [
        "\n",
        "\n",
        "### Q 1.3 [3 points]\n",
        "Could there be a reason to **not** choose the best return according to this ranking when learning off-policy?  Explain your answer.\n",
        "\n",
        "Yes.Even though adaptive_bootstrapping has the lowest average value error, it may not always be the best choice for learning:\n",
        "1. Adaptive_bootstrapping's reliance on dynamic λₜ can introduce instability during training, especially when importance weights are small.\n",
        "2. Complex computation and sensitivity on small fluctuations in ρₜ leads to negative learning effect.\n",
        "3. Although adaptive_bootstrapping reduces estimation error, it may introduce high variance during updates.\n",
        "4. Lower value error does not always mean better policy learning. Simpler estimators may perform better in practice.\n",
        "5. High-variance updates may lead to unstable learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5Xn8bDNFm-H"
      },
      "source": [
        "## Section B: Potential different algorithms/updates [19 points total]\n",
        " Consider a MDP $M = (\\mathbb{S}, \\mathbb{A}, p, r, \\gamma)$ and a behaviour policy $\\mu$. We use policy $\\mu$ to generate trajectories of experience:\n",
        "\\begin{equation*}\n",
        "    (s_{t}, a_{t}, r_{t},s_{t+1}, a_{t+1}, r_{t+1},\\cdots, s_{t+n-1}, a_{t+n-1}, r_{t+n-1}, s_{t+n}, a_{t+n}) \\,.\n",
        "\\end{equation*}\n",
        "Note that this is an $n$-step sequence, starting from time $t$.\n",
        "\n",
        "Given these partial trajectories we consider the following learning problems:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vc8FfT06lYY"
      },
      "source": [
        "### Q2.1 [9 points]\n",
        "Consider a learning update based on the following temporal difference error:\n",
        "$$\\delta_t = R(S_t, A_t) + \\gamma R(S_{t+1}, A_{t+1}) + \\gamma^2 \\max_a q(S_{t+2}, a) - q(S_t, A_t)$$\n",
        "\n",
        "Consider updating a tabular action value function with TD.\n",
        "\n",
        "i) Does the resulting value function converge, under any initialisation of the value function? Consider an appropiate learning rate (Robbins–Monro conditions). If so, prove the convergence under infinity number of interactions with this MDP, under fixed behaviour policy $\\mu$ and show its convergence point. If not, show why it diverges. (7 points)\n",
        "\n",
        "ii) Under which conditions, would the above process converge to the optimal value function $q_*$ ? (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3H4p8jZj6bGP"
      },
      "source": [
        "i) Yes, the value function will converge under suitable conditions. Uunder following assumptions, the value function $q_t$ will converge:\n",
        "\n",
        "\n",
        "#### Assumptions\n",
        "\n",
        "- The state space S and action space A are finite.\n",
        "- The learning rate satisfies the **Robbins-Monro conditions**:\n",
        "\n",
        "$$\n",
        "\\sum_t \\alpha_t(s,a) = \\infty, \\quad \\sum_t \\alpha_t^2(s,a) < \\infty\n",
        "$$\n",
        "\n",
        "- The behaviour policy $ \\mu $ ensures sufficient exploration:\n",
        "\n",
        "$$\n",
        "\\mu(a|s) > 0 \\quad \\forall (s,a) \\in \\mathcal{S} \\times \\mathcal{A}\n",
        "$$\n",
        "\n",
        "- The reward function $ R(s,a) $ is bounded, and the initial value $ q_0(s,a) $ is bounded.\n",
        "\n",
        "\n",
        "#### TD Update Rule\n",
        "\n",
        "The update rule is:\n",
        "\n",
        "$$\n",
        "q_{t+1}(S_t, A_t) = q_t(S_t, A_t) + \\alpha_t \\left[ R(S_t, A_t) + \\gamma R(S_{t+1}, A_{t+1}) + \\gamma^2 \\max_a q_t(S_{t+2}, a) - q_t(S_t, A_t) \\right]\n",
        "$$\n",
        "\n",
        "This defines a stochastic approximation process. According to **Jaakkola et al. (1994)**, convergence is guaranteed under the assumptions above because:\n",
        "\n",
        "- The expected update operator is a contraction:\n",
        "  \n",
        "$$\n",
        "\\| \\mathbb{E}[F_t] \\|_\\infty \\leq \\gamma^2 \\| q_t - q^* \\|_\\infty\n",
        "$$\n",
        "\n",
        "- The variance of the update is bounded.\n",
        "- All state–action pairs are visited infinitely often.\n",
        "\n",
        "\n",
        "**Convergence Point**:\n",
        "\n",
        "$$\n",
        "q^\\dagger = (\\mathcal{T}_Q^*)^2 q\n",
        "$$\n",
        "\n",
        "This is the fixed point of the **double application of the Bellman optimality operator** $ \\mathcal{T}_Q^* $.  \n",
        "It is **not necessarily equal** to the true optimal value \\$ q^* $, but it is the stable fixed point under this TD update rule.\n",
        "\n",
        "---\n",
        "\n",
        "ii) Following TD update process will converge to the optimal value function $q^*$:\n",
        "- The **behavior policy $\\mu$ is the same as the target policy** or sufficient **off-policy corrections** used to account for the difference between behavior and target policy.\n",
        "- The **reward function $R(s, a)$** is bounded.\n",
        "- The **state space $\\mathcal{S}$** and **action space $\\mathcal{A}$** are finite.\n",
        "- The **learning rate $\\alpha_t(s, a)$** satisfies the Robbins–Monro conditions:\n",
        "  $$\n",
        "  \\sum_t \\alpha_t(s, a) = \\infty, \\quad \\sum_t \\alpha_t^2(s, a) < \\infty\n",
        "  $$\n",
        "\n",
        "Under these conditions, the value function $q_t(s,a)$ converges to the **true optimal value function** $q^*$, which is the unique fixed point of the Bellman optimality operator:\n",
        "$$\n",
        "\\mathcal{T}_Q^* f(s, a) = R(s, a) + \\gamma \\sum_{s'} p(s'|s,a) \\max_{a'} f(s', a')\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfIqVjrP6QI-"
      },
      "source": [
        "### Q2.2 [10 points]\n",
        "\n",
        "Consider the same questions now for the following temporal difference error\n",
        "\\begin{equation}\n",
        "        \\delta_t = r(S_{t},A_{t}) + \\gamma \\frac{\\pi(A_{t+1}|S_{t+1})}{\\mu(A_{t+1}|S_{t+1})} \\left[ r(S_{t+1},A_{t+1}) + \\gamma \\max_{a} q(S_{t+2},a) \\right] - q(S_t, A_t)\n",
        "\\end{equation}\n",
        "\n",
        "where $\\pi(a|s) \\in \\arg\\max_a q(s,a), \\forall s,a \\in \\mathbb{A} \\times \\mathbb{S}$ and consider the behaviour policy to be either:\n",
        "\n",
        "  a. $\\mu(a|s) \\in \\arg\\max_a q(s,a), \\forall s,a \\in \\mathbb{A} \\times \\mathbb{S}$,\n",
        "  \n",
        "  b. $\\mu(a|s) = \\frac{1}{|\\mathbb{A}|}$ (uniformly random policy).\n",
        "\n",
        "Answer the below two questions for **both choices** of the behaviour policy $\\mu$:\n",
        "* i)  Does updating a tabular action value function with this TD error converge to the optimal value function $q_*$? Consider an appropiate learning rate (Robbins–Monro conditions). If so, prove this convergence under infinity number of interaction with this MDP, under behaviour policy $\\mu$. If not, show why it diverges or alternatively convergence to a different solution. (4 points)\n",
        "* ii) How does the variance of this update compare to the one induced by the error in Q2.1? (3 points).\n",
        "* iii) Can you propose a different behaviour policy that achieves a lower variance than any of the choices we considered for $\\mu$ (a and b)? Prove that your behaviour policy achieve this. Argue why, if that is not possible. (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGqQJRzD6Tww"
      },
      "source": [
        "i) No, the update using this TD error does not generally converge to the optimal value function $q^*$, although it may converge to a fixed point under standard assumptions.\n",
        "\n",
        "#### Given TD Error with importance sampling correction):\n",
        "$$\n",
        "\\delta_t = r(s_t, a_t) + \\gamma \\frac{\\pi(a_{t+1} \\mid s_{t+1})}{\\mu(a_{t+1} \\mid s_{t+1})} \\left[ r(s_{t+1}, a_{t+1}) + \\gamma^2 \\max_a q(s_{t+2}, a) \\right] - q(s_t, a_t)\n",
        "$$\n",
        "\n",
        "Update rule:\n",
        "$$\n",
        "q_{t+1}(s_t, a_t) = q_t(s_t, a_t) + \\alpha_t \\delta_t\n",
        "$$\n",
        "\n",
        "### Proof:\n",
        "\n",
        "#### 1. Convergence in stochastic approximation sense:\n",
        "Under the following conditions, convergence is guaranteed:\n",
        "- Finite state/action spaces: $|\\mathcal{S}| < \\infty, |\\mathcal{A}| < \\infty$\n",
        "- Robbins–Monro learning rate:\n",
        "$$\n",
        "\\sum_t \\alpha_t(s, a) = \\infty, \\quad \\sum_t \\alpha_t^2(s, a) < \\infty\n",
        "$$\n",
        "- Sufficient exploration: $\\mu(a|s) > 0 \\ \\forall s,a$\n",
        "\n",
        "Then, $q_t$ converges almost surely.\n",
        "\n",
        "\n",
        "#### 2. But convergence is not to $q^*$\n",
        "\n",
        "Because the target includes importance sampling corrections and 2-step bootstrapping, it does **not** directly estimate the Bellman optimal operator $\\mathcal{T}_Q^*$, but rather:\n",
        "$$\n",
        "q^\\dagger = (\\mathcal{T}_Q^*)^2 q\n",
        "$$\n",
        "This is not guaranteed to be equal to $q^*$, unless the behavior policy $\\mu = \\pi$, which convert off-policy to on-policy.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "- If $\\mu \\ne \\pi$: Converges to a biased fixed point $q^\\dagger \\ne q^*$\n",
        "- If $\\mu = \\pi$: On-policy learning; converges to $q^*$\n",
        "\n",
        "---\n",
        "\n",
        "ii)\n",
        "#### Q2.1 TD Error:\n",
        "\n",
        "$$\n",
        "\\delta^{\\text{Q2.1}}_t = r_t + \\gamma \\max_a q(s_{t+1}, a) - q(s_t, a_t)\n",
        "$$\n",
        "\n",
        "This is the standard 1-step **Q-learning** update, which has:\n",
        "- **Low variance**, because it uses the greedy action $a = \\arg\\max_a q(s_{t+1}, a)$, avoiding stochasticity from sampling actions.\n",
        "- **High bias**, especially in the off-policy setting.\n",
        "\n",
        "#### Q2.2 TD Error (importance sampling + 2-step):\n",
        "\n",
        "$$\n",
        "\\delta^{\\text{Q2.2}}_t = r_t + \\gamma \\frac{\\pi(a_{t+1} \\mid s_{t+1})}{\\mu(a_{t+1} \\mid s_{t+1})} \\left( r_{t+1} + \\gamma \\max_a q(s_{t+2}, a) \\right) - q(s_t, a_t)\n",
        "$$\n",
        "\n",
        "This TD error:\n",
        "- Involves **two steps** of sampling: from $s_{t+1}$ and $s_{t+2}$.\n",
        "- Uses **importance sampling ratio** $\\rho = \\frac{\\pi}{\\mu}$, which increases variance, especially when $\\mu$ is small and $\\pi$ is large.\n",
        "- Thus, variance is **significantly higher** than in Q2.1.\n",
        "\n",
        "\n",
        "> The TD error in Q2.2 has **higher variance** than that in Q2.1, due to both the use of **importance sampling corrections** and the **multi-step target**, which compounds noise from rewards and stochastic policies.\n",
        "\n",
        "---\n",
        "iii)\n",
        "Yes, it is possible to design a behaviour policy μ with lower variance than both greedy and uniform.\n",
        "\n",
        "**ε-soft behaviour policy** defined as:\n",
        "\n",
        "$$\n",
        "\\mu(a|s) = \\epsilon \\cdot \\frac{1}{|\\mathcal{A}|} + (1 - \\epsilon) \\cdot \\pi(a|s), \\quad \\epsilon \\in (0,1)\n",
        "$$\n",
        "\n",
        "This behaviour policy is a mixture between the target policy π and the uniform distribution.\n",
        "\n",
        "- When ε → 0: μ → π (on-policy, lowest variance).\n",
        "- When ε → 1: μ → uniform (highest variance).\n",
        "- When 0 < ε < 1: importance ratios $\\rho = \\frac{\\pi(a|s)}{\\mu(a|s)}$ are smaller than with μ = uniform.\n",
        "\n",
        "The variance of $\\rho \\cdot G_t$ is lower because $\\rho(a|s)$ is closer to 1, reducing the effect of importance sampling, which leads to more stable learning and smaller error in off-policy updates.\n",
        "\n",
        "Therefore, ε-soft policy perform better between exploration and low variance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wesi6S866Lyq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}